import seaborn as sns
import pandas as pd
import numpy as np
import sklearn.ensemble._forest
from pandas import datetime
from matplotlib import pyplot as plt
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PowerTransformer
from sklearn.preprocessing import QuantileTransformer
from sklearn.utils import resample



def bootstrapping_ci(train_X, train_Y, test_X, estimator, n_bootstraps = 100,  p_samples = 0.5):
    """
    bootstrapping confidence interval

    Parameters
    ----------
    train_X: pd.DataFrame or numpy.array
        The original features of training set
        
    train_Y: pd.DataFrame or numpy.array
        The original targets of training set
        
    test_X: pd.DataFrame or numpy.array
        The original features of testing set
        
    estimator: sklearn.BaseEstimator
        The estimator to fit and for generating prediction
    
    n_bootstraps: int, default = 100
        The number of resamples taken by bootstrapping
    
    p_samples: float, default = 0.5
        The proportion of random sampling takes from original training data
        
    """
    
    b_x = []
    b_y = []
    prediction_bs = []
    for _ in range(n_bootstraps):
        sample_X ,sample_y = resample(train_X, train_Y, n_samples = int(trian_X.shape[0]*p_samples))
        b_x.append(sample_X)
    b_y.append(sample_y)
    """Now fit the estimators and generate different predictions n_bootstraps times"""
    for i, feature in enumerate(b_x):
        estimator.fit(feature, b_y[i])
        prediction_bs.append(estimator.predict(pftest))
    
    return prediction_bs

def power_trans(df):

    """
    Distribution transformation
    
    Parameters
    ----------
    df: pd.DataFrame
        The dataframe to be power transfromed.
        
    Returns
    -------
    df_trans: pd.DataFrame
        The dataframe after power transformation.

    """
    pt = PowerTransformer(method = "yeo-johnson")
    pt.fit(df)
    df_trans = pd.DataFrame(pt.transform(df), columns=train.columns[:])
    return df_trans


def quantile_trans(df, n):
    """
    Distribution transformation
    
    Parameters
    ----------
    df: pd.DataFrame
        The dataframe to be power transfromed.

    n: int
        The number of quantiles for quantile transformation.
        
    Returns
    -------
    df_trans: pd.DataFrame
        The dataframe after quantile transformation.

    """
    rng = np.random.RandomState(304)
    qf = QuantileTransformer(n_quantiles=n, output_distribution='normal',random_state=rng)
    qt.fit(df)
    df_trans = pd.DataFrame(qt.transform(df), columns=train.columns)
    return df_trans

def lag_ahead_series(data, n_in=1, n_out=1, n_vars = 1, dropnan=True, nskip = 0):
    """ 
    convert one to multiple series
    
    Parameters
    ----------
    data: pd.DataFrame
        The time series data frame to create lag or ahead series.

    n_in: int, default = 1
        The number of lags to be created.

    n_out: int, default = 1
        The number of aheads to be created.

    n_vars: int, default = 1
        The number of time series features to be fed in.

    dropnan: bool, default = True
        Whether to drop the nan generated by shifting backward and forward.

    nskip: int, default = 0
        The width of gap when creating every lag/ahead time series.
    
    Returns
    -------
    agg: pd.DataFrame
        The created lag/ahead time series.

    """
    df = pd.DataFrame(data)
    cols, names = list(), list()
    if n_in + n_out == 0:
        return
    # skipped sequences (t + nskip, ... t + n)
    if nskip != 0:
        # input sequence (t-n, ..., t-1) 
        for j in range(n_vars):
            for i in range(n_in, 0, -nskip):
                cols.append(df.iloc[:,j].shift(i))
                names.append('{}{}(t-{})'.format(df.columns[j], j+1, i))
        # forecast sequence (t+1, ..., t+n)
        for j in range(n_vars):
            for i in np.arange(0, n_out, nskip):
                cols.append(df.iloc[:,j].shift(-i))
                names += [('{}{}(t+{})'.format(df.columns[j], j+1, i))]
    # regular sequences
    else:
        # input sequence (t-n, ... t-1)
        for j in range(n_vars):
            for i in range(n_in, 0, -1):
                cols.append(df.iloc[:,j].shift(i))
                names.append('{}{}(t-{})'.format(df.columns[j], j+1, i))

        # forecast sequence (t+1, ... t+n)
        for j in range(n_vars):
            for i in range(0, n_out):
                cols.append(df.iloc[:,j].shift(-i))
                names += [('{}{}(t+{})'.format(df.columns[j], j+1, i))]
    # put it all together
    agg = pd.concat(cols, axis=1)
    agg.columns = names
    
    #drop rows with NaN values
    if dropnan:
        agg.dropna(inplace=True)
    return agg

def tfla_series(data, n_in=1, n_out=1, n_vars = 1, dropnan=True, nskip = 0):
    """ 
    convert one to multiple series, with variable name compatible for tsfresh feature extraction.
    
    Parameters
    ----------
    data: pd.DataFrame
        The time series data frame to create lag or ahead series.

    n_in: int, default = 1
        The number of lags to be created.

    n_out: int, default = 1
        The number of aheads to be created.

    n_vars: int, default = 1
        The number of time series features to be fed in.

    dropnan: bool, default = True
        Whether to drop the nan generated by shifting backward and forward.

    nskip: int, default = 0
        The width of gap when creating every lag/ahead time series.
    
    Returns
    -------
    agg: pd.DataFrame
        The created lag/ahead time series.

    """
    df = pd.DataFrame(data)
    cols, names = list(), list()
    if n_in + n_out == 0:
        return
    # skipped sequences (t + nskip, ... t + n)
    if nskip != 0:
        # input sequence (t-n, ..., t-1) 
        for j in range(n_vars):
            for i in range(n_in, 0, -nskip):
                cols.append(df.iloc[:,j].shift(i))
                names.append('{}'.format(-i+n_in))
        # forecast sequence (t+1, ..., t+n)
        for j in range(n_vars):
            for i in np.arange(0, n_out, nskip):
                cols.append(df.iloc[:,j].shift(-i))
                names += [('{}{}(t+{})'.format(df.columns[0], j+1, i))]
    # regular sequences
    else:
        # input sequence (t-n, ... t-1)
        for j in range(n_vars):
            for i in range(n_in, 0, -1):
                cols.append(df.iloc[:,j].shift(i))
                names.append('{}'.format(-i+n_in))

        # forecast sequence (t+1, ... t+n)
        for j in range(n_vars):
            for i in range(0, n_out):
                cols.append(df.iloc[:,j].shift(-i))
                names += [('{}{}(t+{})'.format(df.columns[0], j+1, i))]
    # put it all together
    agg = pd.concat(cols, axis=1)
    agg.columns = names
    
    #drop rows with NaN values
    if dropnan:
        agg.dropna(inplace=True)
    return agg

def tf_construct(df, target_name, load_lag, target_ahead):
    load = df['%s'% target_name]
    ## load lag series
    f = tfla_series(load,
                  n_in = load_lag,
                  n_out = 0,
                  n_vars = 1,
                  dropnan = True)
    ## target part
    t = tfla_series(load,
                  n_in = 0,
                  n_out = target_ahead,
                  n_vars = 1,
                  dropnan = True)
    # alignment of feature and target
    f, t = f.align(t, 'inner', axis = 0)
    
    return f, t


def get_eval(y, yhat):
    """
    get evaluation metrics of the prediciton
    
    Parameters
    ----------
    y: numpy.array or pd.DataFrame [n_samples, n_targets]
        The observed data

    yhat: numpy.array or pd.DataFrame [n_samples, n_targets]
        The prediction
    

    """
    print("MSE: {}".format(mean_squared_error(y,yhat)))
    print("MAE: {}".format(mean_absolute_error(y,yhat)))
    print("r2_score: {}".format(r2_score(y,yhat, multioutput = "variance_weighted")))
    
def extract_dmhq(df):
    """
    Extract day of week, day of month, month, minute, hour, day of year of given DataFrame with time stamp.
    
    Parameters
    ----------
    df: pd.DataFrame with timestamp as index.

    Returns
    -------
    df: pd.DataFrame
        The new dataframe with new time-oriented features.

    """
    df['date'] = df.index.astype('datetime64[ns]')
    df['wd'] = df['date'].dt.dayofweek 
    df['month'] = df['date'].dt.month
    df['day'] = df['date'].dt.day
    df['hour'] = df['date'].dt.hour
    df['minute'] = df['date'].dt.minute
    df['yd'] = df['date'].dt.dayofyear
    df.drop(['date'], axis = 1, inplace = True)
    return df

def feature_target_construct(df, target_name, load_lag, target_ahead, temp_lag, temp_ahead, f_picked, tskip = 0, wd_on = False, d_on = False, m_on = False, h_on = False, q_on = False):
    """
    feature/ target construction fucntion with lag variable

    Parameters
    ----------
    df: pd.DataFrame
        The dataframe to be constructed with lag, ahead

    target_name: str
        The target to be tranformed into time series lag and ahead
        form.

    load_lag: int
        The number of lags to be created as features.

    target_ahead: int
        The number of ahead to be created as targets.

    temp_lag: int
        The number of lag weather features

    temp_ahead: int
        The number of ahead weather features(usually forecasting data)

    f_picked: list with str
        A list decide which weather features to be included as features.

    tskip: int, default = 0
        The width of gaps between lag/ahead weather features.
    
    wd_on: boolean, default = False
        switch determines whether include weekday or not.

    d_on: boolean, default = False
        switch determines whether include day of month or not.
        
    m_on: boolean, default = False
        switch determines whether include month or not.

    h_on: boolean, default = False
        switch determines whether include hour or not.

    q_on: boolean, default = False
        switch determines whether include quarter or not.
    
    Returns
    -------
    f: pd.DataFrame
        The feature constructed.

    t: pd.DataFrame
        The target constructed.

    """
    tempcols = f_picked 
    load = df['%s'% target_name]
    f_temp = pd.DataFrame()
    
    ## temp ahead series
    for col in tempcols:
        if(tskip != 0):
            temp = lag_ahead_series(df[col], 
                                 n_in = temp_lag,
                                 n_out = temp_ahead,
                                 n_vars = 1,
                                 dropnan = True,
                                 nskip= tskip)
        else:
             temp = lag_ahead_series(df[col], 
                                 n_in = temp_lag,
                                 n_out = temp_ahead,
                                 n_vars = 1,
                                 dropnan = True)
        f_temp = pd.concat([f_temp, temp], axis = 1)            
        
    ## load lag series
    f_load = lag_ahead_series(load,
                          n_in = load_lag,
                          n_out = 0,
                          n_vars = 1,
                          dropnan = True)
    # when f_temp exist
    if f_temp.shape[1] > 0:
        f_load, f_temp = f_load.align(f_temp, 'inner', axis = 0)
        f = pd.concat([f_temp, f_load], axis = 1)
    # when no f_temp
    else:
        f = f_load
        
    ## hour one hot on
    if h_on:
        # month one hot encoding
        hour = df['hour']
        # alignment
        hour , f = hour.align(f, 'inner', axis = 0)
        f = pd.concat([hour, f], axis = 1)

    ## quarter one hot on
    if q_on:
        # month one hot encoding
        minute = df['minute']
        # alignment
        minute , f = minute.align(f, 'inner', axis = 0)
        f = pd.concat([minute, f], axis = 1)

    ## weekday one hot on
    if wd_on:
        # weekday one hot encoding
        weekday = df['wd']
        # alignment
        weekday , f = weekday.align(f, 'inner', axis = 0)
        f = pd.concat([weekday, f], axis = 1)
        
    ## day one hot on
    if d_on:
        # day one hot encoding
        day = df['day']
        # alignment
        day , f = day.align(f, 'inner', axis = 0)
        f = pd.concat([day, f], axis = 1)
    
    ## month one hot on
    if m_on:
        # month one hot encoding
        month = df['month']
        # alignment
        month , f = month.align(f, 'inner', axis = 0)
        f = pd.concat([month, f], axis = 1)
        
        
    ## number of LCLid on                  
    if('LCLid' in df.columns):
        nlclid = df['LCLid']
        # alignment
        nlclid, f = nlclid.align(f, 'inner', axis = 0)       
        f = pd.concat([f, nlclid], axis = 1)
        
        
    ## target part
    t = lag_ahead_series(load,
                          n_in = 0,
                          n_out = target_ahead,
                          n_vars = 1,
                          dropnan = True)
    # alignment of feature and target
    f, t = f.align(t, 'inner', axis = 0)
    
    return f, t

def atomic_predict(ensemble_estimator, test_X):
    """
    Atomically predict model generated in autosklearn ensemble

    Parameters
    ----------
    ensemble_estimator : autosklearn.estimators.AutoSklearnRegressor
        The final ensemble_estimator. Should be refitted if cross validation is used.
    
    test_X : pd.DataFrame or np.array
        The feature of testing set
    
    Returns
    -------
    weights: list
        The weights of each model in voting ensemble

    predictions: list
        The prediction of each model, forest based estimator maps to nested list
        with prediction of each sub tree in the forest.
    """
    model_list = ensemble_estimator.get_models_with_weights()
    weights = []
    predictions = []
    for i in range(len(model_list)):
        weights.append(model_list[i][0])
        single_pred = []
        # if forest based estimator, construct nested prediction for each tree
        if issubclass(type(model_list[i][1][2].choice.estimator), \
                      sklearn.ensemble._forest.BaseForest):
            for est in model_list[i][1][2].choice.estimator.estimators_:
                pip = Pipeline([('data_pre', model_list[i][1][0]), \
                                ('feature_pre', model_list[i][1][1])])
                ptest_X = pip.transform(test_X)
                single_pred.append(est.predict(ptest_X))
            predictions.append(single_pred)
        else:
            predictions.append(model_list[i][1].predict(test_X))
    return weights, predictions

def autosk_ci(weights, predictions, ci, t, normal=False): 
    """
    Confidence interval based on autosklearn ensemble model with BaseForest estimators.

    Parameters
    ----------
    weights : list
        The list contains weights of the voting ensemble of autosklearn result

    predictions : list
        The list contains atomic prediction of each model in emsemble pool

    ci : float, (0, 1)
        The precentage of confidence interval

    t : int
        The timestamp of examining prediction 

    normal : boolean, default = False
        The assumption of distribution of prediction is normal or not

    Returns
    _______
    ub : np.array
        The upper bond of confidence interval
    lb : np.array
        The lower bond of confidence interval
        
    """
    ub = 0
    lb = 0
    for i, weight in enumerate(weights):
       # check if the prediciton is list 
        if issubclass(type(predictions[i]), list):
            ub += weight * np.quantile(predict_list[i], 1 - (1-ci)/2, axis = 0)[t,:]
            lb += weight * np.quantile(predict_list[i], (1-ci)/2, axis = 0)[t,:]
        else:
            ub += weight * predictions[i]
            lb += weight * predictions[i]
    return ub, lb
